{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from Omodel import swin_small_patch4_window7_224, BaggageClassifier #,QuantityClassifier\n",
    "from Odataset import PersonWithBaggageDataset, TRAIN_CSV_FILE, TEST_CSV_FILE, ROOT_DIR, TRAIN_TRANSFORM, VAL_TRANSFORM\n",
    "from torchvision.transforms import v2 as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class UnNormalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be un-normalized.\n",
    "        Returns:\n",
    "            Tensor: Un-normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "# Example mean and std values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class QuantityClassifier(nn.Module):\n",
    "    '''\n",
    "    num_classes: 0, 1, 2, >=3\n",
    "    '''\n",
    "    def __init__(self,c_in=768,num_classes = 4, pool='avg'):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        if pool == 'avg':\n",
    "            self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        elif pool == 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(c_in, c_in//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(c_in//4, (c_in//4)//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear( (c_in//4)//4, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, feature, label=None):\n",
    "        feat = self.pool(feature).view(feature.size(0), -1)\n",
    "        x = self.logits(feat)\n",
    "        return x\n",
    "    \n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BEST_MODEL_PATH = '/home/deepvisionpoc/Desktop/Jeans/SOLIDER_exp/SOLIDER-PersonAttributeRecognition/Oxygen_runs/run_20240712_162058/checkpoint.pth.tar'\n",
    "unnormalize = UnNormalize(mean, std)\n",
    "\n",
    "FAKE_VAL_TRANSFORM = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the dataset\n",
    "test_ds = PersonWithBaggageDataset(TEST_CSV_FILE, ROOT_DIR, FAKE_VAL_TRANSFORM)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Prepare the model\n",
    "backbone = swin_small_patch4_window7_224()\n",
    "classifier = QuantityClassifier()\n",
    "model = BaggageClassifier(backbone, classifier).to(device)\n",
    "\n",
    "# Load the best model\n",
    "ckpt = torch.load(BEST_MODEL_PATH)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(f\"Loaded Model Details:\\nEpoch: {ckpt['epoch']} Acc: {ckpt['best_acc']}\")\n",
    "\n",
    "# Function to visualize images and predictions\n",
    "def visualize_predictions(images, labels, predictions, class_names):\n",
    "    plt.figure(figsize=(14, 16))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        img = (img + 1)/2\n",
    "        print(img.min())\n",
    "        \n",
    "        print(img.max())\n",
    "        # img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {class_names[labels[i].item()]}\\nPred: {class_names[predictions[i].item()]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Perform inference and visualize results\n",
    "model.eval()\n",
    "class_names = ['No Bag', '1 Bag', '2 Bag', 'At Least 3 Bag']  # Update this with actual class names\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Inference\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predictions = outputs.max(1)\n",
    "        \n",
    "        \n",
    "        visualize_predictions(images, labels, predictions, class_names)\n",
    "        count+=1  # Remove this to visualize all batches, currently visualizes only the first batch\n",
    "        if count == 1:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepvisionpoc/miniconda3/envs/jeans/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 2024-07-01T10-25-48_FLAG_P_FID_290_OID_148g0_ctw-cf-2c-094_1719804300_1719804348_290.jpg: 'choices'\n",
      "Error processing 2024-07-01T10-37-15_FLAG_P_FID_816_OID_574g0_ctw-cf-3c-163_1719804900_1719805035_816.jpg: 'choices'\n",
      "Error processing 2024-07-01T10-48-00_FLAG_P_FID_1083_OID_1185g0_ctw-cf-1c-049_1719805500_1719805680_1083.jpg: 'choices'\n",
      "Error processing 2024-07-01T11-20-54_FLAG_P_FID_324_OID_530g0_ctw-cf-1e-072_1719807601_1719807654_324.jpg: 'choices'\n",
      "Error processing 2024-07-01T12-13-34_FLAG_P_FID_1280_OID_286g0_ctw-cf-3c-169_1719810601_1719810814_1280.jpg: 'choices'\n",
      "Error processing 2024-07-01T13-04-02_FLAG_P_FID_1456_OID_109g0_ctw-cf-2b-146_1719813600_1719813842_1456.jpg: 'choices'\n",
      "Error processing 2024-07-01T13-53-25_FLAG_P_FID_1230_OID_1099g0_ctw-cf-3c-156_1719816601_1719816805_1230.jpg: 'choices'\n",
      "Error processing 2024-07-01T14-27-11_FLAG_P_FID_793_OID_1158g0_ctw-cf-2a-138_1719818700_1719818831_793.jpg: 'choices'\n",
      "Error processing 2024-07-01T14-42-14_FLAG_P_FID_810_OID_1946g0_ctw-cf-1c-019-store_1719819600_1719819734_810.jpg: 'choices'\n",
      "Error processing 2024-07-01T14-48-54_FLAG_P_FID_1408_OID_1762g0_ctw-cf-2c-103_1719819900_1719820134_1408.jpg: 'choices'\n",
      "Error processing 2024-07-01T15-11-36_FLAG_P_FID_582_OID_634g0_ctw-cf-2a-140_1719821400_1719821496_582.jpg: 'choices'\n",
      "Error processing 2024-07-01T15-29-10_FLAG_P_FID_1507_OID_1893g0_ctw-cf-2c-093_1719822300_1719822550_1507.jpg: 'choices'\n",
      "Error processing 2024-07-01T15-32-20_FLAG_P_FID_837_OID_1828g0_ctw-cf-3a-185_1719822601_1719822740_837.jpg: 'choices'\n",
      "Error processing 2024-07-01T15-49-21_FLAG_P_FID_1567_OID_698g0_ctw-cf-1b-007_1719823501_1719823761_1567.jpg: 'choices'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "from torch.utils.data import DataLoader\n",
    "from Omodel import swin_small_patch4_window7_224, BaggageClassifier  # ,QuantityClassifier\n",
    "from Odataset import GPTDataset, TRAIN_CSV_FILE, TEST_CSV_FILE, ROOT_DIR, TRAIN_TRANSFORM, VAL_TRANSFORM\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "FAKE_VAL_TRANSFORM = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "output_dir = 'output/fig-gpt'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a DataLoader for the GPTDataset\n",
    "dataset = GPTDataset(transform=FAKE_VAL_TRANSFORM, use_expected_value=False)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Iterate over the batches\n",
    "count = 0\n",
    "for batch_index, (imgs, targetTop1s,logProbTop1s, targetEVs, _, img_paths) in enumerate(dataloader):\n",
    "    # Convert the batch of images to a NumPy array\n",
    "    imgs = imgs.numpy().transpose(0, 2, 3, 1)  # (batch_size, height, width, channels)\n",
    "    imgs = (imgs + 1.0) / 2.0\n",
    "\n",
    "    # Create a figure with subplots for each image\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "    # Iterate over the images and labels\n",
    "    for i, (img, top, logprob,ev,img_name) in enumerate(zip(imgs, targetTop1s, logProbTop1s,targetEVs,img_paths)):\n",
    "        count += 1\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        axs[row, col].imshow(img)\n",
    "        axs[row, col].set_title(f\"{\"_\".join(img_name.split(\"_\")[1:-3])}\\n{count} ({exp(logprob):.3f})(Top1, EV): {top}, {ev}\")\n",
    "        axs[row, col].axis('off')\n",
    "\n",
    "    # Save the plot\n",
    "    fig_filename = os.path.join(output_dir, f\"fig_{batch_index + 1}.png\")\n",
    "    plt.savefig(fig_filename)\n",
    "    plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "    # # Break after the first 8 batches\n",
    "    if batch_index >= 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
