{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Omodel import swin_small_patch4_window7_224, QuantityClassifier, QuantityClassifierV2, QuantityClassifierV3, BaggageClassifier\n",
    "from Odataset import create_GPT_train_test_loader, PersonWithBaggageDataset\n",
    "from PIL import Image\n",
    "from kornia.losses import binary_focal_loss_with_logits, focal_loss\n",
    "# Load the model checkpoint\n",
    "\n",
    "\n",
    "\n",
    "classes_weight=torch.tensor([ 1.49175036 , 0.42052578 , 1.08703607, 31.50757576])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BEST_MODEL_PATH = 'Oruns/GPT/focal_loss_no_weight/BiggerLR_07-17:17:51_E32_[2e-05,0.0001-min1e-05]/model_best.pth.tar'\n",
    "BEST_MODEL_PATH = 'Oruns/GPT/focal_loss_no_weight/BiggerLR_07-17:18:00_E32_[1e-05,0.001-min1e-05]/model_best.pth.tar'\n",
    "ckpt = torch.load(BEST_MODEL_PATH)\n",
    "\n",
    "# Define the function to plot images with true and predicted labels\n",
    "def plot_images_with_labels(data_loader, model, device, num_images=5):\n",
    "    model.eval()\n",
    "    to_pil = ToPILImage()\n",
    "    targets = [0,0,0,0] \n",
    "    preds = [0,0,0,0] \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targetTop1s, _, _, _, img_paths) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            targetTop1s = targetTop1s.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Plot the images with their true and predicted labels\n",
    "            for j in range(min(num_images, len(images))):\n",
    "                # plt.figure(figsize=(10, 5))\n",
    "                true_label = targetTop1s[j].item()\n",
    "                pred_label = predicted[j].item()\n",
    "                targets[true_label] +=1 \n",
    "                preds[pred_label] +=1 \n",
    "                img = Image.open(img_paths[j]).convert(\"RGB\")\n",
    "                \n",
    "                # plt.imshow(img)\n",
    "                # plt.title(f'True Label: {true_label}, Predicted Label: {pred_label}')\n",
    "                # plt.axis('off')\n",
    "                # plt.show()\n",
    "\n",
    "            if i >= 20:\n",
    "                break\n",
    "    print(f\"correct: {preds}\")\n",
    "    print(f\"outof  : {targets}\")\n",
    "    \n",
    "\n",
    "# Prepare the dataset & model\n",
    "train_loader, test_loader = create_GPT_train_test_loader(BATCH_SIZE=64)\n",
    "backbone = swin_small_patch4_window7_224()\n",
    "classifier = QuantityClassifierV3()\n",
    "model = BaggageClassifier(backbone, classifier).to(device)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Plot some images with their true and predicted labels\n",
    "plot_images_with_labels(test_loader, model, device, num_images=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
